{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0c83a7",
   "metadata": {},
   "source": [
    "# 1. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold, RepeatedStratifiedKFold, GridSearchCV\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ee5f8",
   "metadata": {},
   "source": [
    "# 2. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(\"../gandalf-doe/test_data/dataset.xlsx\", index_col=0, sheet_name=\"Training\")\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_excel(\"../gandalf-doe/test_data/dataset.xlsx\", index_col=0, sheet_name=\"Test\")\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c31927",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
    "all_samples.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94608dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determined by Mahalanobis distance\n",
    "\n",
    "inter_train = all_samples.drop([5, 9, 12, 13, 16, 17, 21, 31, 37, 43])\n",
    "inter_test = all_samples.loc[[5, 9, 12, 13, 16, 17, 21, 31, 37, 43]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92074d04",
   "metadata": {},
   "source": [
    "# 3. Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef255598",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Temperature', 'Pressure', 'GHSV', 'Ni', 'Co', 'Calcination', 'Reduction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f00035",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_max = np.array([7.63e+02, 1.00e+01, 2.64e+04, 2.50e+01, 1.00e+01, 9.23e+02, 9.23e+02])\n",
    "scale_min = np.array([5.23e+02, 1.00e+00, 3.30e+03, 0.00e+00, 0.00e+00, 6.23e+02, 6.23e+02])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbaadc6",
   "metadata": {},
   "source": [
    "# 4. Nested Repeated k-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {\n",
    "    'booster': ['gbtree'],                    # Booster type\n",
    "    'n_estimators': [1000, 5000, 10000],\n",
    "                                              # Number of trees\n",
    "    'max_depth': [2, 3, 4],                   # Depth of each tree\n",
    "    'learning_rate': [0.01, 0.05, 0.25, 0.5, 0.75],\n",
    "                                              # Learning rate\n",
    "    'subsample': [0.1, 0.5, 0.6, 0.7, 1],        \n",
    "                                              # Fraction of samples used per tree\n",
    "    'tree_method': ['hist', 'exact'],         # Method to grow trees\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],         # Number of trees\n",
    "    'max_depth': [None, 2, 5],                # Depth of each tree\n",
    "    'max_features': ['sqrt', 'log2'],         # Maximal features\n",
    "    'min_samples_split': [2, 5, 10],          # Minimal sample split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation_conversion(model, param_grid, X, y, n_splits=8, n_repeats=4):\n",
    "    start_t = time.time()\n",
    "    new_t = start_t\n",
    "    outer_results = {\"INDEX\": [], \"PRED\": [], \"TRUE\": []}\n",
    "\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=n_splits, random_state=120897, n_repeats=n_repeats)\n",
    "\n",
    "    for train_index, test_index in outer_cv.split(X, np.digitize(y, np.percentile(y, np.arange(0, 100, 10)))):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        inner_cv = KFold(n_splits=n_splits, shuffle=True, random_state=210995)\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv, scoring='r2', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = np.clip(best_model.predict(X_test), 0, 100)\n",
    "        print(y_pred)\n",
    "        \n",
    "        # Store the results\n",
    "        outer_results[\"INDEX\"] += list(test_index)\n",
    "        outer_results[\"PRED\"] += list(y_pred)\n",
    "        outer_results[\"TRUE\"] += list(y_test)\n",
    "        print(\"Finished a round after \", time.time()-new_t)\n",
    "        new_t = time.time()\n",
    "\n",
    "    # Compute final metrics on the outer results\n",
    "    res = pd.DataFrame(outer_results)\n",
    "    true_data = res[\"TRUE\"].to_numpy()\n",
    "    pred_data = res[\"PRED\"].to_numpy()\n",
    "\n",
    "    mae = mean_absolute_error(true_data, pred_data)\n",
    "    rmse = np.sqrt(mean_squared_error(true_data, pred_data))\n",
    "    r2 = r2_score(true_data, pred_data)\n",
    "\n",
    "    print(f\"Nested CV Performance: MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")\n",
    "    \n",
    "    return mae, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation_sty(model, param_grid, X, y, ghsv, n_splits=8, n_repeats=4):\n",
    "    outer_results = {\"INDEX\": [], \"PRED\": [], \"TRUE\": []}\n",
    "\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=n_splits, random_state=120897, n_repeats=n_repeats)\n",
    "\n",
    "    for train_index, test_index in outer_cv.split(X, np.digitize(y, np.percentile(y, np.arange(0, 100, 10)))):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        ghsv_test = ghsv[test_index]\n",
    "\n",
    "        inner_cv = KFold(n_splits=n_splits, shuffle=True, random_state=210995)\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv, scoring='r2', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = np.clip(best_model.predict(X_test), 0, 100)\n",
    "        print(y_pred)\n",
    "        preds = (y_pred / 100) * ghsv_test * (12.011 + 4*1.0079) / (5*22400)\n",
    "        trues = (y_test / 100) * ghsv_test * (12.011 + 4*1.0079) / (5*22400)\n",
    "        \n",
    "        # Store the results\n",
    "        outer_results[\"INDEX\"] += list(test_index)\n",
    "        outer_results[\"PRED\"] += list(preds)\n",
    "        outer_results[\"TRUE\"] += list(trues)\n",
    "\n",
    "    # Compute final metrics on the outer results\n",
    "    res = pd.DataFrame(outer_results)\n",
    "    true_data = res[\"TRUE\"].to_numpy()\n",
    "    pred_data = np.clip(res[\"PRED\"].to_numpy(), 0, 100)\n",
    "\n",
    "    mae = mean_absolute_error(true_data, pred_data)\n",
    "    rmse = np.sqrt(mean_squared_error(true_data, pred_data))\n",
    "    r2 = r2_score(true_data, pred_data)\n",
    "\n",
    "    print(f\"Nested CV Performance: MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")\n",
    "    \n",
    "    return mae, rmse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3d000",
   "metadata": {},
   "source": [
    "## 4.1 Conversion with XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88bdb56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaled_input = (df_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "nested_cross_validation_conversion(xgb.sklearn.XGBRegressor(random_state=0), xgb_param_grid, scaled_input, df_train[\"Conversion\"].to_numpy(), n_splits=8, n_repeats=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e9d66",
   "metadata": {},
   "source": [
    "## 4.2 Conversion with RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_input = (df_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "nested_cross_validation_conversion(RandomForestRegressor(random_state=0), rf_param_grid, scaled_input, df_train[\"Conversion\"].to_numpy(), n_splits=8, n_repeats=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c419d",
   "metadata": {},
   "source": [
    "## 4.3 STY with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_input = (df_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "nested_cross_validation_sty(xgb.sklearn.XGBRegressor(random_state=0), xgb_param_grid, scaled_input, df_train[\"Conversion\"].to_numpy(), df_train[\"GHSV\"].to_numpy(), n_splits=8, n_repeats=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d423291",
   "metadata": {},
   "source": [
    "## 4.4 STY with RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e745c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_input = (df_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "nested_cross_validation_sty(RandomForestRegressor(random_state=0), rf_param_grid, scaled_input, df_train[\"Conversion\"].to_numpy(), df_train[\"GHSV\"].to_numpy(), n_splits=8, n_repeats=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f38ed",
   "metadata": {},
   "source": [
    "# 5. Interpolation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbece40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_external_dataset_on_conversion(model, X_train, y_train, X_test, y_test, param_grid, n_splits):\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=210995)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='r2', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best params\", grid_search.best_params_)\n",
    "    y_pred = np.clip(best_model.predict(X_test), 0, 100)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return mae, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_external_dataset_on_sty(model, X_train, y_train, X_test, y_test, param_grid, n_splits, ghsv):\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=210995)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='r2', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best params\", grid_search.best_params_)\n",
    "    y_pred = np.clip(best_model.predict(X_test), 0, 100)\n",
    "    preds = (y_pred / 100) * ghsv * (12.011 + 4*1.0079) / (5*22400)\n",
    "    trues = (y_test / 100) * ghsv * (12.011 + 4*1.0079) / (5*22400)\n",
    "                                \n",
    "    mae = mean_absolute_error(trues, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
    "    r2 = r2_score(trues, preds)\n",
    "    return mae, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1424f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "scaled_train_input = (inter_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "scaled_test_input = (inter_test[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "test_external_dataset_on_conversion(xgb.sklearn.XGBRegressor(random_state=0), \n",
    "                      scaled_train_input, inter_train[\"Conversion\"].to_numpy(), \n",
    "                      scaled_test_input, inter_test[\"Conversion\"].to_numpy(), \n",
    "                      xgb_param_grid, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7cf179",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_input = (inter_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "scaled_test_input = (inter_test[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "test_external_dataset_on_conversion(RandomForestRegressor(random_state=0), \n",
    "                      scaled_train_input, inter_train[\"Conversion\"].to_numpy(), \n",
    "                      scaled_test_input, inter_test[\"Conversion\"].to_numpy(), \n",
    "                      rf_param_grid, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5aa3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_input = (inter_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "scaled_test_input = (inter_test[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "test_external_dataset_on_sty(xgb.sklearn.XGBRegressor(random_state=0), \n",
    "                             scaled_train_input, inter_train[\"Yield\"].to_numpy(), \n",
    "                             scaled_test_input, inter_test[\"Yield\"].to_numpy(), \n",
    "                             xgb_param_grid, 8, inter_test[\"GHSV\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf45e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_input = (inter_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "scaled_test_input = (inter_test[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "test_external_dataset_on_sty(RandomForestRegressor(random_state=0), \n",
    "                             scaled_train_input, inter_train[\"Yield\"].to_numpy(), \n",
    "                             scaled_test_input, inter_test[\"Yield\"].to_numpy(), \n",
    "                             rf_param_grid, 8, inter_test[\"GHSV\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad997ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86312521",
   "metadata": {},
   "source": [
    "# 6. Extrapolation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_input = (df_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "scaled_test_input = (df_test[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "test_external_dataset_on_conversion(xgb.sklearn.XGBRegressor(random_state=0), \n",
    "                                    scaled_train_input, df_train[\"Conversion\"].to_numpy(), \n",
    "                                    scaled_test_input, df_test[\"Conversion\"].to_numpy(), \n",
    "                                    xgb_param_grid, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e87019",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_input = (df_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "scaled_test_input = (df_test[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "test_external_dataset_on_co nversion(RandomForestRegressor(random_state=0), \n",
    "                                    scaled_train_input, df_train[\"Conversion\"].to_numpy(), \n",
    "                                    scaled_test_input, df_test[\"Conversion\"].to_numpy(), \n",
    "                                    rf_param_grid, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ba70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_input = (df_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "scaled_test_input = (df_ b  test[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "test_external_dataset_on_sty(xgb.sklearn.XGBRegressor(random_state=0), \n",
    "                             scaled_train_input, df_train[\"Yield\"].to_numpy(), \n",
    "                             scaled_test_input, df_test[\"Yield\"].to_numpy(), \n",
    "                             xgb_param_grid, 8, df_test[\"GHSV\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_input = (df_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "scaled_test_input = (df_test[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "test_external_dataset_on_sty(RandomForestRegressor(random_state=0), \n",
    "                             scaled_train_input, df_train[\"Yield\"].to_numpy(), \n",
    "                             scaled_test_input, df_test[\"Yield\"].to_numpy(), \n",
    "                             rf_param_grid, 8, df_test[\"GHSV\"].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24e0b0",
   "metadata": {},
   "source": [
    "# 7. SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation_shap(model, param_grid, X, y, n_splits=8, n_repeats=4):\n",
    "    feature_importances = None\n",
    "    outer_results = {\"INDEX\": [], \"PRED\": [], \"TRUE\": []}\n",
    "\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=n_splits, random_state=120897, n_repeats=n_repeats)\n",
    "\n",
    "    for train_index, test_index in outer_cv.split(X, np.digitize(y, np.percentile(y, np.arange(0, 100, 10)))):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        inner_cv = KFold(n_splits=n_splits, shuffle=True, random_state=210995)\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv, scoring='r2', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = np.clip(best_model.predict(X_test), 0, 100)\n",
    "        print(y_pred)\n",
    "        outer_results[\"INDEX\"] += list(test_index)\n",
    "        outer_results[\"PRED\"] += list(y_pred)\n",
    "        outer_results[\"TRUE\"] += list(y_test)\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        explanation = explainer(X_train, check_additivity=False)\n",
    "        feature_importance = np.mean(np.abs(explanation.values), axis=0)\n",
    "        feature_importance = feature_importance / np.sum(feature_importance)\n",
    "        if feature_importances is None:\n",
    "            feature_importances = feature_importance\n",
    "        else:\n",
    "            feature_importances = np.vstack((feature_importances, feature_importance))\n",
    "\n",
    "    # Compute final metrics on the outer results\n",
    "    res = pd.DataFrame(outer_results)\n",
    "    true_data = res[\"TRUE\"].to_numpy()\n",
    "    pred_data = res[\"PRED\"].to_numpy()\n",
    "\n",
    "    mae = mean_absolute_error(true_data, pred_data)\n",
    "    rmse = np.sqrt(mean_squared_error(true_data, pred_data))\n",
    "    r2 = r2_score(true_data, pred_data)\n",
    "    \n",
    "    avg_importances = np.mean(feature_importances, axis=0)\n",
    "    sd_importances = np.std(feature_importances, axis=0)\n",
    "\n",
    "    print(f\"Nested CV Performance: MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")\n",
    "    \n",
    "    return avg_importances, sd_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa82db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_input = (df_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "x_avgs, x_sds = nested_cross_validation_shap(xgb.sklearn.XGBRegressor(random_state=0), xgb_param_grid, scaled_input, df_train[\"Conversion\"].to_numpy(), n_splits=8, n_repeats=4)\n",
    "print(\"Average importance:\", x_avgs)\n",
    "print(\"Standard deviation:\", x_sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_input = (df_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "s_avgs, s_sds = nested_cross_validation_shap(xgb.sklearn.XGBRegressor(random_state=0), xgb_param_grid, scaled_input, df_train[\"CH4 Selectivity\"].to_numpy(), n_splits=8, n_repeats=4)\n",
    "print(\"Average importance:\", s_avgs)\n",
    "print(\"Standard deviation:\", s_sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f952998",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_input = (df_train[variables].to_numpy() - scale_min) / (scale_max - scale_min)\n",
    "sty_avgs, sty_sds = nested_cross_validation_shap(xgb.sklearn.XGBRegressor(random_state=0), xgb_param_grid, scaled_input, df_train[\"STY\"].to_numpy(), n_splits=8, n_repeats=4)\n",
    "print(\"Average importance:\", sty_avgs)\n",
    "print(\"Standard deviation:\", sty_sds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpnn3d",
   "language": "python",
   "name": "mpnn3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
